
# This is the ReadMe for the Data used in the project

### Data provided by Spotlight

[Spotlight](https://maciejkula.github.io/spotlight/index.html) offers 3 different kind of popular rating datasets provided as modules: 

1. [**Synthetic**](https://maciejkula.github.io/spotlight/datasets/synthetic.html):  this model contains functions for generating synthetic datasets with known properties, for model testing and experimentation. Synthetic data is information that is artificially manufactured rather than generated by real-world events. In this case it is created by a n-th order Markov Chain with uniform stationary distribution. 
1. [**Movielens**](https://maciejkula.github.io/spotlight/datasets/movielens.html): MovieLens dataset is a collection of ratings from movies. Collections have been made over various periods of time depending the dataset. Spotlight provides a module to extract the data in several datasets: 100K, 1M, 10M, and 20M. 
1. [**Goodbooks**](https://maciejkula.github.io/spotlight/datasets/goodbooks.html): This dataset contains six million ratings for ten thousand most popular (with most ratings) books. The module fetches the [Goodbooks-10K dataset](https://github.com/zygmuntz/goodbooks-10k). 


### Extra data

The data provided by Spotlight is alegable to be used for the training and testing of the model, nevertheless as it is provided in predefined Spotlight modules it is very hard to do a pre-analysis of the data alone.

Because one of the tasks of the project corresponds to analyzing the data, we have decided to use data from [The Movies Dataset](https://www.kaggle.com/datasets/rounakbanik/the-movies-dataset) available on Kaggle to do so. Some of the models will be trained from this data and others will be trained directly from the data provided by Spotlight descrived above.

The Movies Dataset contains files with 26 million ratings from 270,000 users for 45,000 movies. For the purpose of this project, we used the ratings_small file from the dataset, which contains a subset of 100,000 ratings from 700 users on 9,000 movies. The full dataset takes a much longer time to train on but you can try using it if you have a machine with a powerful GPU. 

We will use 3 data files:

- **ratings_small.csv** — contains the rating data for different users and movies.
- **movies_metadata.csv** — contains the metadata for all the 45,000 movies in the dataset.
- **links.csv** — contains the IDs that can be used to lookup each movie when joining this data with the movie metadata.

In this folder we have created 2 jupyter notebooks:
- *initial_metrics_data.ipynb*: has the code for the plots and the analysis of the data
- *preprocess.ipynb*: has the code for preprocessing the data, looking for NaN and imputing them
